# Directories
output_folder: !ref <output_root>/<lang_id>_transformer/
save_folder: !ref <output_folder>/save/
log_file: !ref <output_folder>/logs/train_log.txt
results_folder: !ref results/<lang_id>_transformer/
train_csv: !ref <train_csv_file>
valid_csv: !ref <valid_csv_file>
wer_output_dir: !ref <results_folder>/wer.txt

# Modules
modules: None



# Tokenizer & LM
text_file: !ref manifests/<lang_id>_text.txt
tokenizer_file: !ref <output_folder>/tokenizer.ckpt
lm_model_path: null  


# Audio & Features
sample_rate: 16000
normalize: True
input_feat: raw
channels: 1

# Model
emb_size: 512
hidden_size: 2048
num_layers: 6
nhead: 8
dropout: 0.1

# Training
number_of_epochs: 1
batch_size: 8
gradient_accumulation: 2
lr: 1.0
lr_warmup: 500
max_grad_norm: 5.0

# Checkpoints
checkpoints_dir: !ref <save_folder>/checkpoints
checkpoint_interval: 1
keep_checkpoints: 3
save_optimizer_state: True
auto_resume: True

# Decoding / Evaluation
use_language_model: False
wer_file: !ref <results_folder>/wer.txt
cer_file: !ref <results_folder>/cer.txt

# Pretrained model (from HuggingFace)
pretrained_path: speechbrain/asr-transformer-transformerlm-librispeech

# Dynamic values to inject from script
lang_id: ???
train_csv_file: ???
valid_csv_file: ???
output_root: model
output_neurons: 1024
opt_class: !name:torch.optim.Adam
  lr: 0.0001
epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter
  limit: !ref <number_of_epochs>

