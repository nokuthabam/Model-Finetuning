# Directories
output_folder: !ref <output_root>/<lang_id>_transformer/
save_folder: !ref <output_folder>/save/
log_file: !ref <output_folder>/logs/train_log.txt
results_folder: !ref results/<lang_id>_transformer/
train_csv: !ref <train_csv_file>
valid_csv: !ref <valid_csv_file>

# Modules
modules: None
  # encoder: !new:speechbrain.lobes.models.transformer.TransformerASR.Encoder
  #   input_size: 80
  #   d_model: !ref <emb_size>
  #   nhead: !ref <nhead>
  #   num_layers: !ref <num_layers>
  #   d_ffn: !ref <hidden_size>
  #   dropout: !ref <dropout>
  # decoder: !new:speechbrain.lobes.models.transformer.TransformerASR.Decoder
  #   d_model: !ref <emb_size>
  #   nhead: !ref <nhead>
  #   num_layers: !ref <num_layers>
  #   d_ffn: !ref <hidden_size>
  #   dropout: !ref <dropout>
  #   vocab_size: !ref <output_neurons>
  # ctc_lin: !new:torch.nn.Linear
  #   in_features: !ref <emb_size>
  #   out_features: !ref <output_neurons>



# Tokenizer & LM
text_file: !ref manifests/<lang_id>_text.txt
tokenizer_file: !ref <output_folder>/tokenizer.ckpt
lm_model_path: null  # Optional: you can point to a trained LM if you have one


# Audio & Features
sample_rate: 16000
normalize: True
input_feat: raw
channels: 1

# Model
emb_size: 512
hidden_size: 2048
num_layers: 6
nhead: 8
dropout: 0.1

# Training
number_of_epochs: 10
batch_size: 8
gradient_accumulation: 2
lr: 1.0
lr_warmup: 500
max_grad_norm: 5.0

# Checkpoints
checkpoints_dir: !ref <save_folder>/checkpoints
checkpoint_interval: 1
keep_checkpoints: 3
save_optimizer_state: True
auto_resume: True

# Decoding / Evaluation
use_language_model: False
wer_file: !ref <results_folder>/wer.txt
cer_file: !ref <results_folder>/cer.txt

# Pretrained model (from HuggingFace)
pretrained_path: speechbrain/asr-transformer-transformerlm-librispeech

# Dynamic values to inject from script
lang_id: ???
train_csv_file: ???
valid_csv_file: ???
output_root: model
output_neurons: 1024
opt_class: !name:torch.optim.Adam
  lr: 0.0001
epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter
  limit: !ref <number_of_epochs>

